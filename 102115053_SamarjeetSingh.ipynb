{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samarjeet22/102115053-SESS_LE1/blob/main/102115053_SamarjeetSingh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name: **Samarjeet Singh**  \n",
        "Email: `ssingh10_be21@thapar.edu`  \n",
        "Roll No: **102115053**  \n",
        "Group: **4NC2**  \n",
        "Start Timestamp: 20240911-1000"
      ],
      "metadata": {
        "id": "JSYdjLETIt2o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question\n",
        " Consider the paper: <https://arxiv.org/abs/1804.03209>\n",
        "\n",
        "  1. Read and summarise the paper in about 50 words.\n",
        "  2. Download the dataset in the paper, statistically analyse and\n",
        "     describe it, so that it may be useful for posterity. (Include code\n",
        "     snippets in your .ipynb file to evidence your analysis.)\n",
        "  3. Train a classifier so that you are able to distinguish the commands\n",
        "     in the dataset.\n",
        "  4. Report the performance results using standard benchmarks.\n",
        "  5. Record about 30 samples of each command in your voice and create a\n",
        "     new dataset (including a new user id for yourself).  You may use a\n",
        "     timer on your computer to synchronise.\n",
        "  6. Fine tune your classifier to perform on your voice.\n",
        "  7. Report the results."
      ],
      "metadata": {
        "id": "pn0MNsVQLGNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SUMMARY\n",
        "The paper \"Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition\" introduces a dataset to aid in training keyword spotting systems. It details how the data was collected, the types of words included, and the challenges in creating models for recognizing specific words from background noise efficiently. The paper also discusses methods for evaluation, the importance of lightweight models for on-device processing, and provides baseline results for comparing future models. The dataset is open-source, enabling broad usage and advancements in speech recognition research."
      ],
      "metadata": {
        "id": "u7sbhDPHMtX7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ktSGB8dsIsy4",
        "outputId": "cef50547-204c-4ef3-aaaa-aa744fc347fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Collecting sounddevice\n",
            "  Downloading sounddevice-0.5.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Downloading sounddevice-0.5.0-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: sounddevice\n",
            "Successfully installed sounddevice-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchaudio torch sounddevice\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torchaudio\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "# Create the data directory if it doesn't exist\n",
        "data_dir = './data'\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "# Download the Speech Commands dataset\n",
        "dataset = torchaudio.datasets.SPEECHCOMMANDS(root=data_dir, download=True)\n"
      ],
      "metadata": {
        "id": "2LEtBihRSy6t",
        "outputId": "42f6db01-1357-47e9-d36b-4acb6368d0da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.26G/2.26G [01:45<00:00, 23.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select 10 commands to work with\n",
        "selected_commands = ['dog', 'cat', 'up', 'down', 'wow', 'yes', 'go', 'stop', 'on', 'off']\n",
        "\n",
        "# Limit the number of samples per command (e.g., 100 samples per command)\n",
        "samples_per_command = 100\n"
      ],
      "metadata": {
        "id": "5VVzV90gS4W-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a subset of the dataset by filtering for the selected commands\n",
        "subset_indices = []\n",
        "command_counter = Counter()\n",
        "\n",
        "for idx, sample in enumerate(dataset):\n",
        "    label = sample[2]\n",
        "    if label in selected_commands and command_counter[label] < samples_per_command:\n",
        "        subset_indices.append(idx)\n",
        "        command_counter.update([label])\n",
        "\n",
        "    # Stop when we have enough samples for each command\n",
        "    if all(command_counter[cmd] >= samples_per_command for cmd in selected_commands):\n",
        "        break\n",
        "\n",
        "# Create a subset of the dataset\n",
        "subset_dataset = Subset(dataset, subset_indices)\n",
        "\n",
        "# Check the sample count for each command in the subset\n",
        "print(f\"Sample counts in subset: {command_counter}\")\n",
        "print(f\"Total subset size: {len(subset_dataset)}\")"
      ],
      "metadata": {
        "id": "jRILhrfDVc4Y",
        "outputId": "33586f01-94bd-45a6-bf70-2ad42bbbc691",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample counts in subset: Counter({'cat': 100, 'dog': 100, 'down': 100, 'go': 100, 'off': 100, 'on': 100, 'stop': 100, 'up': 100, 'wow': 100, 'yes': 100})\n",
            "Total subset size: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "tnatfZtJeHbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Preprocessing (Padding and Truncating)\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n"
      ],
      "metadata": {
        "id": "Fzou5HxAWN1U"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a fixed length for all audio samples (1 second = 16000 samples at 16kHz)\n",
        "fixed_length = 16000\n",
        "\n",
        "# Custom collate function to pad and truncate audio data\n",
        "def collate_fn(batch):\n",
        "    waveforms = []\n",
        "    labels = []\n",
        "\n",
        "    for item in batch:\n",
        "        waveform = item[0]\n",
        "        label = item[2]\n",
        "\n",
        "        if waveform.shape[1] > fixed_length:\n",
        "            waveform = waveform[:, :fixed_length]\n",
        "        elif waveform.shape[1] < fixed_length:\n",
        "            pad_amount = fixed_length - waveform.shape[1]\n",
        "            waveform = torch.nn.functional.pad(waveform, (0, pad_amount))\n",
        "\n",
        "        waveforms.append(waveform)\n",
        "        labels.append(label)\n",
        "\n",
        "    waveforms = torch.stack(waveforms)\n",
        "    return waveforms, labels"
      ],
      "metadata": {
        "id": "VNf6u0Lqd3ZX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader for the subset dataset\n",
        "loader = DataLoader(subset_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "mo4b0lDTd575"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CNN CLASSIFIER"
      ],
      "metadata": {
        "id": "Mtjn7vgsd-mR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define and Train a CNN Classifier (with correct fully connected layer input size)\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchaudio.transforms as transforms\n",
        "\n",
        "# Define the MelSpectrogram transform to convert audio waveforms into spectrograms\n",
        "mel_spectrogram = transforms.MelSpectrogram(\n",
        "    sample_rate=16000, n_mels=128, n_fft=400, hop_length=160\n",
        ")\n",
        "\n",
        "# Define a simple CNN model for speech command classification\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(32 * 25 * 32, 128)  # Correct input size based on shape (32*25=800)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layers\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Get the number of classes (commands)\n",
        "num_classes = len(selected_commands)\n",
        "\n",
        "# Create a dictionary to map commands (labels) to numerical values\n",
        "label_to_index = {label: idx for idx, label in enumerate(selected_commands)}\n",
        "\n",
        "# Function to convert string labels to numerical indices\n",
        "def label_to_tensor(label):\n",
        "    return torch.tensor(label_to_index[label])\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleCNN(num_classes=num_classes).to('cuda')\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop for 10 epochs\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for waveforms, labels in loader:\n",
        "        # Convert waveforms to spectrograms\n",
        "        waveforms = mel_spectrogram(waveforms)\n",
        "        waveforms = waveforms.squeeze(1).unsqueeze(1).to('cuda')  # Remove extra dimension, add channel dimension\n",
        "        labels = torch.tensor([label_to_tensor(label) for label in labels]).to('cuda')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(waveforms)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(loader)}')\n",
        "\n",
        "print('Training completed!')"
      ],
      "metadata": {
        "id": "tHSRAQjmd9CG",
        "outputId": "c825f7ac-7ffd-4333-8d44-5e8fe7229b50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 3.4963080808520317\n",
            "Epoch 2/10, Loss: 2.012493845075369\n",
            "Epoch 3/10, Loss: 1.5896470323204994\n",
            "Epoch 4/10, Loss: 1.1217845231294632\n",
            "Epoch 5/10, Loss: 0.8777167685329914\n",
            "Epoch 6/10, Loss: 0.6619481900706887\n",
            "Epoch 7/10, Loss: 0.4038319722749293\n",
            "Epoch 8/10, Loss: 0.33765738760121167\n",
            "Epoch 9/10, Loss: 0.37012161873281\n",
            "Epoch 10/10, Loss: 0.17110937379766256\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MODEL EVALUATION"
      ],
      "metadata": {
        "id": "BxMQajI_eXaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the Model\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation for faster evaluation\n",
        "    for waveforms, labels in loader:  # Assuming you're using the same loader for validation\n",
        "        waveforms = mel_spectrogram(waveforms)\n",
        "        waveforms = waveforms.squeeze(1).unsqueeze(1).to('cuda')\n",
        "        labels = torch.tensor([label_to_tensor(label) for label in labels]).to('cuda')\n",
        "\n",
        "        outputs = model(waveforms)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy: {accuracy}%')"
      ],
      "metadata": {
        "id": "6k79KpWReVlM",
        "outputId": "4404de54-b371-407c-f48c-ea34d4e4e010",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 98.7%\n"
          ]
        }
      ]
    }
  ]
}